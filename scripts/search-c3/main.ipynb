{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "344c3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import bigquery\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19023e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 373\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select distinct search_internal_keyword\n",
    "from `rome-prod.temp.search_internal_keyword_view_2025` \n",
    "where (ds_category_ids is null or lower(ds_category_ids) in ('',' ','na','null','n/a','-'))\n",
    "and  (search_internal_keyword in (select distinct search_internal_keyword from `rome-prod.temp.search_internal_keyword_20260109_v2` where lower(status) <> 'ok' )\n",
    "or  search_internal_keyword in (select distinct search_internal_keyword from `rome-prod.temp.search_internal_keyword_20260109_v1` where lower(status) <> 'ok' ))\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "print(f\"Total records: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079866dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"checkpoints/batch_1.csv\")\n",
    "\n",
    "destination_table = \"rome-prod.temp.\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df, destination_table, job_config=job_config)\n",
    "job.result()\n",
    "\n",
    "print(f\"Loaded {len(df)} rows to {destination_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "617b6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://ds-search-c3-prediction.qa2-sg.cld/predict/search-term-data\"\n",
    "\n",
    "def get_error_result(search_term, error_type, error_msg):\n",
    "    return {\n",
    "        \"search_internal_keyword\": search_term,\n",
    "        \"code\": None,\n",
    "        \"status\": f\"{error_type}: {error_msg}\",\n",
    "        \"isGenericTerm\": None,\n",
    "        \"isValidC3\": None,\n",
    "        \"score\": None,\n",
    "        \"c1CategoryCode\": None,\n",
    "        \"c2CategoryCode\": None,\n",
    "        \"c3CategoryCode\": None,\n",
    "        \"c3_name\": None,\n",
    "        \"c3_category\": None\n",
    "    }\n",
    "\n",
    "def call_predict_api(search_term):\n",
    "    if not search_term or pd.isna(search_term):\n",
    "        return get_error_result(search_term, \"SKIP\", \"Empty or null search term\")\n",
    "    \n",
    "    data = {\n",
    "        \"searchTerm\": search_term,\n",
    "        \"categoryPredCutoff\": 30,\n",
    "        \"requestId\": \"\",\n",
    "        \"maxCategoryHierarchy\": 1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = session.post(API_URL, json=data, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        response_json = resp.json()\n",
    "        \n",
    "        result = {\n",
    "            \"search_internal_keyword\": search_term,\n",
    "            \"code\": response_json.get(\"code\"),\n",
    "            \"status\": response_json.get(\"status\"),\n",
    "            \"isGenericTerm\": None,\n",
    "            \"isValidC3\": None,\n",
    "            \"score\": None,\n",
    "            \"c1CategoryCode\": None,\n",
    "            \"c2CategoryCode\": None,\n",
    "            \"c3CategoryCode\": None,\n",
    "            \"c3_name\": None,\n",
    "            \"c3_category\": None\n",
    "        }\n",
    "        \n",
    "        if response_json.get(\"data\"):\n",
    "            data_obj = response_json[\"data\"]\n",
    "            result[\"isGenericTerm\"] = data_obj.get(\"isGenericTerm\")\n",
    "            result[\"isValidC3\"] = data_obj.get(\"isValidC3\")\n",
    "            \n",
    "            if data_obj.get(\"categoryHierarchyList\") and len(data_obj[\"categoryHierarchyList\"]) > 0:\n",
    "                cat = data_obj[\"categoryHierarchyList\"][0]\n",
    "                result[\"score\"] = cat.get(\"score\")\n",
    "                result[\"c1CategoryCode\"] = cat.get(\"c1CategoryCode\")\n",
    "                result[\"c2CategoryCode\"] = cat.get(\"c2CategoryCode\")\n",
    "                result[\"c3CategoryCode\"] = cat.get(\"c3CategoryCode\")\n",
    "                result[\"c3_name\"] = cat.get(\"c3_name\")\n",
    "                result[\"c3_category\"] = cat.get(\"c3_category\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return get_error_result(search_term, \"TIMEOUT\", \"Request timed out after 30s\")\n",
    "    \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return get_error_result(search_term, \"CONNECTION_ERROR\", \"Failed to connect to API\")\n",
    "    \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return get_error_result(search_term, \"HTTP_ERROR\", str(e))\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        return get_error_result(search_term, \"JSON_ERROR\", \"Invalid JSON response\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return get_error_result(search_term, \"ERROR\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f146ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique search terms: 373\n",
      "\n",
      "==================================================\n",
      "START TIME: 2026-01-09 15:37:46\n",
      "==================================================\n",
      "\n",
      "Batch 1/1 - Processing 373 terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 373/373 [00:32<00:00, 11.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: checkpoints/batch_1.csv\n",
      "Batch 1 uploaded: 373 rows | Success: 365 | Failed: 8 | Time: 36.33s\n",
      "\n",
      "==================================================\n",
      "END TIME: 2026-01-09 15:38:23\n",
      "TOTAL TIME: 0h 0m 36.34s\n",
      "TOTAL RECORDS: 373\n",
      "SUCCESS: 365 (97.86%)\n",
      "FAILED: 8 (2.14%)\n",
      "AVG SPEED: 10.27 records/sec\n",
      "CHECKPOINTS: checkpoints/\n",
      "==================================================\n",
      "\n",
      "Completed! All data uploaded to rome-prod.temp.search_internal_keyword_20260109_v2\n"
     ]
    }
   ],
   "source": [
    "search_terms = df[\"search_internal_keyword\"].dropna().unique().tolist()\n",
    "print(f\"Unique search terms: {len(search_terms)}\")\n",
    "\n",
    "NUM_THREADS = 40\n",
    "BATCH_SIZE = 50000\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "RESUME_FROM_BATCH = 0  # Set to batch number to resume from (0 = start fresh)\n",
    "\n",
    "destination_table = \"rome-prod.temp.search_internal_keyword_20260109_v2\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "total_batches = (len(search_terms) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"START TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "if RESUME_FROM_BATCH > 0:\n",
    "    print(f\"RESUMING FROM BATCH: {RESUME_FROM_BATCH + 1}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "total_success = 0\n",
    "total_failed = 0\n",
    "\n",
    "for batch_idx in range(RESUME_FROM_BATCH, total_batches):\n",
    "    batch_start = time.time()\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min((batch_idx + 1) * BATCH_SIZE, len(search_terms))\n",
    "    batch_terms = search_terms[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\nBatch {batch_idx + 1}/{total_batches} - Processing {len(batch_terms)} terms\")\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "        futures = {executor.submit(call_predict_api, term): term for term in batch_terms}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Batch {batch_idx + 1}\"):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    batch_success = (results_df[\"code\"] == 200).sum()\n",
    "    batch_failed = (results_df[\"code\"] != 200).sum()\n",
    "    total_success += batch_success\n",
    "    total_failed += batch_failed\n",
    "\n",
    "    \n",
    "    checkpoint_file = f\"{CHECKPOINT_DIR}/batch_{batch_idx + 1}.csv\"\n",
    "    results_df.to_csv(checkpoint_file, index=False)\n",
    "    print(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "    \n",
    "    if batch_idx == 0 and RESUME_FROM_BATCH == 0:\n",
    "        job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    else:\n",
    "        job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "    \n",
    "    job = client.load_table_from_dataframe(results_df, destination_table, job_config=job_config)\n",
    "    job.result()\n",
    "    \n",
    "    batch_elapsed = time.time() - batch_start\n",
    "    print(f\"Batch {batch_idx + 1} uploaded: {len(results_df)} rows | Success: {batch_success} | Failed: {batch_failed} | Time: {batch_elapsed:.2f}s\")\n",
    "    \n",
    "    with open(f\"{CHECKPOINT_DIR}/progress.txt\", \"w\") as f:\n",
    "        f.write(f\"last_completed_batch={batch_idx + 1}\\n\")\n",
    "        f.write(f\"total_batches={total_batches}\\n\")\n",
    "        f.write(f\"timestamp={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    \n",
    "    del results, results_df\n",
    "\n",
    "end_time = time.time()\n",
    "total_elapsed = end_time - start_time\n",
    "\n",
    "hours, remainder = divmod(total_elapsed, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"END TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"TOTAL TIME: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "print(f\"TOTAL RECORDS: {len(search_terms)}\")\n",
    "print(f\"SUCCESS: {total_success} ({100*total_success/len(search_terms):.2f}%)\")\n",
    "print(f\"FAILED: {total_failed} ({100*total_failed/len(search_terms):.2f}%)\")\n",
    "print(f\"AVG SPEED: {len(search_terms)/total_elapsed:.2f} records/sec\")\n",
    "print(f\"CHECKPOINTS: {CHECKPOINT_DIR}/\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nCompleted! All data uploaded to {destination_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cigarette",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
